{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4eb8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in D:/assignment 2/CSV1.csv: Index(['Unnamed: 0', 'HADM_ID', 'SHORT-TEXT', 'ICD9_CODE', 'ICD9', 'Label'], dtype='object')\n",
      "Column names in D:/assignment 2/CSV2.csv: Index(['Unnamed: 0', 'HADM_ID', 'TEXT', 'LABLE', 'entites', 'group'], dtype='object')\n",
      "Column names in D:/assignment 2/CSV3.csv: Index(['HADM_ID', 'TEXT', 'ICD9_CODE', 'Label'], dtype='object')\n",
      "Column names in D:/assignment 2/CSV4.csv: Index(['HADM_ID', 'TEXT', 'LABLE'], dtype='object')\n",
      "Combined texts saved to D:/assignment 2/combined_txt.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_files = [\n",
    "    'D:/assignment 2/CSV1.csv',\n",
    "    'D:/assignment 2/CSV2.csv',\n",
    "    'D:/assignment 2/CSV3.csv',\n",
    "    'D:/assignment 2/CSV4.csv'\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        print(f\"Column names in {file}: {df.columns}\")\n",
    "\n",
    "        text_column = None\n",
    "        for col in df.columns:\n",
    "            if 'text' in col.lower():\n",
    "                text_column = col\n",
    "                break\n",
    "\n",
    "        if text_column is not None:\n",
    "            texts = df[text_column].astype(str)\n",
    "            all_texts.extend(texts)\n",
    "        else:\n",
    "            print(f\"Warning: No text column found in {file}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Warning: Empty file found - {file}\")\n",
    "\n",
    "if all_texts:\n",
    "    combined_text = \"\\n\".join(all_texts)\n",
    "    output_file = 'D:/assignment 2/combined_txt.txt'\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_text)\n",
    "\n",
    "    print(f'Combined texts saved to {output_file}')\n",
    "else:\n",
    "    print('No valid text columns found in the CSV files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba630b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and: 2341409\n",
      "the: 2158529\n",
      "to: 1931537\n",
      "of: 1926215\n",
      "was: 1867807\n",
      "with: 1366827\n",
      "a: 1151764\n",
      "on: 1104052\n",
      "in: 1000773\n",
      "for: 976439\n",
      "is: 775103\n",
      "mg: 725803\n",
      "no: 552340\n",
      "patient: 525955\n",
      "The: 497153\n",
      "at: 496295\n",
      "or: 479400\n",
      "Tablet: 452050\n",
      "as: 450188\n",
      "PO: 408981\n",
      "(1): 381504\n",
      "Sig:: 371957\n",
      "**]: 371169\n",
      "Name: 364713\n",
      "were: 347112\n",
      "he: 346640\n",
      "his: 346495\n",
      "her: 329535\n",
      "left: 326477\n",
      "had: 321160\n",
      "Top 30 words and their counts saved to D:/assignment 2/top30_words.csv\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "file_path = 'D:/assignment 2/combined_txt.txt'\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "words = text.split()\n",
    "\n",
    "word_counts = Counter(words)\n",
    "\n",
    "top_30_words = word_counts.most_common(30)\n",
    "\n",
    "for word, count in top_30_words:\n",
    "    print(f'{word}: {count}')\n",
    "    \n",
    "csv_file_path = 'D:/assignment 2/top30_words.csv'\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Word', 'Count']) \n",
    "    csv_writer.writerows(top_30_words)\n",
    "print(f'Top 30 words and their counts saved to {csv_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7baec-5035-48d5-b32e-0c78c3d485e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "def count_unique_tokens(text, model_name='bert-base-uncased'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_counts = Counter(tokens)\n",
    "    return token_counts.most_common(30)\n",
    "\n",
    "text_file_path = 'D:/assignment 2/combined_txt.txt'\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "with open(text_file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "top_30_tokens = count_unique_tokens(text)\n",
    "print(top_30_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c960bb9e-f1ac-4927-947d-677df0314d8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_sci_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men_core_sci_sm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_entities\u001b[39m(text):\n\u001b[0;32m      6\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(text)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\scispacy\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\scispacy\\lib\\site-packages\\spacy\\util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_sci_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\"diseases\": [], \"drugs\": []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DISEASE\":\n",
    "            entities[\"diseases\"].append(ent.text)\n",
    "        elif ent.label_ == \"DRUG\":\n",
    "            entities[\"drugs\"].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "file_path = \"D:/assignment 2/combined_txt.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "result = extract_entities(text)\n",
    "\n",
    "print(\"Diseases:\", result[\"diseases\"])\n",
    "print(\"Drugs:\", result[\"drugs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef7417-f794-416b-914c-58454c2c2759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\scispacy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at monologg/biobert_v1.1_pubmed were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at monologg/biobert_v1.1_pubmed and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import spacy\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"monologg/biobert_v1.1_pubmed\", num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "\n",
    "labels = [\"O\", \"B-Disease\", \"I-Disease\"]\n",
    "\n",
    "\n",
    "def extract_entities(text):\n",
    "\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
    "\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    for token, label_id in zip(tokens, predictions):\n",
    "        label = labels[label_id]\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\"label\": label[2:], \"text\": [token]}\n",
    "        elif label.startswith(\"I-\"):\n",
    "            if current_entity is not None:\n",
    "                current_entity[\"text\"].append(token)\n",
    "        else:\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = None\n",
    "\n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    extracted_entities = [{\"label\": entity[\"label\"], \"text\": \" \".join(entity[\"text\"])} for entity in entities]\n",
    "    \n",
    "    return extracted_entities\n",
    "\n",
    "file_path = \"D:/assignment 2/combined_txt.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "result = extract_entities(text)\n",
    "\n",
    "for entity in result:\n",
    "    print(f\"Label: {entity['label']}, Text: {entity['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ffa545-4b74-4d1c-a8f4-35cb77a357a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
