{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4eb8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names in D:/assignment 2/CSV1.csv: Index(['Unnamed: 0', 'HADM_ID', 'SHORT-TEXT', 'ICD9_CODE', 'ICD9', 'Label'], dtype='object')\n",
      "Column names in D:/assignment 2/CSV2.csv: Index(['Unnamed: 0', 'HADM_ID', 'TEXT', 'LABLE', 'entites', 'group'], dtype='object')\n",
      "Column names in D:/assignment 2/CSV3.csv: Index(['HADM_ID', 'TEXT', 'ICD9_CODE', 'Label'], dtype='object')\n",
      "Column names in D:/assignment 2/CSV4.csv: Index(['HADM_ID', 'TEXT', 'LABLE'], dtype='object')\n",
      "Combined texts saved to D:/assignment 2/combined_txt.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_files = [\n",
    "    'D:/assignment 2/CSV1.csv',\n",
    "    'D:/assignment 2/CSV2.csv',\n",
    "    'D:/assignment 2/CSV3.csv',\n",
    "    'D:/assignment 2/CSV4.csv'\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        print(f\"Column names in {file}: {df.columns}\")\n",
    "\n",
    "        text_column = None\n",
    "        for col in df.columns:\n",
    "            if 'text' in col.lower():\n",
    "                text_column = col\n",
    "                break\n",
    "\n",
    "        if text_column is not None:\n",
    "            texts = df[text_column].astype(str)\n",
    "            all_texts.extend(texts)\n",
    "        else:\n",
    "            print(f\"Warning: No text column found in {file}\")\n",
    "\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Warning: Empty file found - {file}\")\n",
    "\n",
    "if all_texts:\n",
    "    combined_text = \"\\n\".join(all_texts)\n",
    "    output_file = 'D:/assignment 2/combined_txt.txt'\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_text)\n",
    "\n",
    "    print(f'Combined texts saved to {output_file}')\n",
    "else:\n",
    "    print('No valid text columns found in the CSV files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba630b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and: 1855246\n",
      "the: 1622439\n",
      "to: 1551252\n",
      "of: 1529701\n",
      "was: 1463004\n",
      "with: 1085118\n",
      "a: 946459\n",
      "on: 846446\n",
      "for: 792433\n",
      "in: 790563\n",
      "is: 590654\n",
      "mg: 589659\n",
      "The: 497153\n",
      "Tablet: 452050\n",
      "no: 414216\n",
      "or: 410997\n",
      "PO: 408981\n",
      "patient: 407361\n",
      "at: 406540\n",
      "(1): 381504\n",
      "as: 375101\n",
      "Sig:: 371957\n",
      "**]: 371169\n",
      "Name: 364713\n",
      "-: 317701\n",
      "He: 299487\n",
      ".: 282627\n",
      "One: 282478\n",
      "BLOOD: 282462\n",
      "were: 282251\n",
      "Top 30 words and their counts saved to D:\\assignment 2\\answer\\Question 1\\top30_words.csv\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "file_path = r'D:\\assignment 2\\answer\\Question 1\\COMBINE_TEXT.txt'\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {file_path}\")\n",
    "    exit()\n",
    "\n",
    "words = text.split()\n",
    "\n",
    "word_counts = Counter(words)\n",
    "\n",
    "top_30_words = word_counts.most_common(30)\n",
    "\n",
    "for word, count in top_30_words:\n",
    "    print(f'{word}: {count}')\n",
    "    \n",
    "csv_file_path = r'D:\\assignment 2\\answer\\Question 1\\top30_words.csv'\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Word', 'Count']) \n",
    "    csv_writer.writerows(top_30_words)\n",
    "print(f'Top 30 words and their counts saved to {csv_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af7baec-5035-48d5-b32e-0c78c3d485e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\jax\\_src\\lib\\__init__.py:24\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jaxlib'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_and_get_top_tokens\u001b[39m(file_path, model_name, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m      6\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\utils\\__init__.py:31\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     24\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     25\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     30\u001b[0m )\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     ContextManagers,\n\u001b[0;32m     33\u001b[0m     ExplicitEnum,\n\u001b[0;32m     34\u001b[0m     ModelOutput,\n\u001b[0;32m     35\u001b[0m     PaddingStrategy,\n\u001b[0;32m     36\u001b[0m     TensorType,\n\u001b[0;32m     37\u001b[0m     add_model_info_to_auto_map,\n\u001b[0;32m     38\u001b[0m     cached_property,\n\u001b[0;32m     39\u001b[0m     can_return_loss,\n\u001b[0;32m     40\u001b[0m     expand_dims,\n\u001b[0;32m     41\u001b[0m     find_labels,\n\u001b[0;32m     42\u001b[0m     flatten_dict,\n\u001b[0;32m     43\u001b[0m     infer_framework,\n\u001b[0;32m     44\u001b[0m     is_jax_tensor,\n\u001b[0;32m     45\u001b[0m     is_numpy_array,\n\u001b[0;32m     46\u001b[0m     is_tensor,\n\u001b[0;32m     47\u001b[0m     is_tf_symbolic_tensor,\n\u001b[0;32m     48\u001b[0m     is_tf_tensor,\n\u001b[0;32m     49\u001b[0m     is_torch_device,\n\u001b[0;32m     50\u001b[0m     is_torch_dtype,\n\u001b[0;32m     51\u001b[0m     is_torch_tensor,\n\u001b[0;32m     52\u001b[0m     reshape,\n\u001b[0;32m     53\u001b[0m     squeeze,\n\u001b[0;32m     54\u001b[0m     strtobool,\n\u001b[0;32m     55\u001b[0m     tensor_size,\n\u001b[0;32m     56\u001b[0m     to_numpy,\n\u001b[0;32m     57\u001b[0m     to_py_obj,\n\u001b[0;32m     58\u001b[0m     transpose,\n\u001b[0;32m     59\u001b[0m     working_or_temp_dir,\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     CLOUDFRONT_DISTRIB_PREFIX,\n\u001b[0;32m     63\u001b[0m     DISABLE_TELEMETRY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m     try_to_load_from_cache,\n\u001b[0;32m     91\u001b[0m )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     93\u001b[0m     ENV_VARS_TRUE_AND_AUTO_VALUES,\n\u001b[0;32m     94\u001b[0m     ENV_VARS_TRUE_VALUES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     torch_required,\n\u001b[0;32m    198\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\utils\\generic.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_flax_available, is_tf_available, is_torch_available, is_torch_fx_proxy\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjnp\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mcached_property\u001b[39;00m(\u001b[38;5;28mproperty\u001b[39m):\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Descriptor that mimics @property but caches output in member variable.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Built-in in functools from Python 3.8.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\jax\\__init__.py:35\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _cloud_tpu_init\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Confusingly there are two things named \"config\": the module and the class.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# We want the exported object to be the class, so we first import the module\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# to make sure a later import doesn't overwrite the class.\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config \u001b[38;5;28;01mas\u001b[39;00m _config_module\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _config_module\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Force early import, allowing use of `jax.core` after importing `jax`.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\jax\\config.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The JAX Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# TODO(phawkins): fix users of this alias and delete this file.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\jax\\_src\\config.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Callable, Hashable, NamedTuple, Iterator, Optional\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jax_jit\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_src\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transfer_guard_lib\n",
      "File \u001b[1;32mc:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\jax\\_src\\lib\\__init__.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mjaxlib\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m---> 26\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjax requires jaxlib to be installed. See \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://github.com/google/jax#installation for installation instructions.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mversion\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _minimum_jaxlib_version \u001b[38;5;28;01mas\u001b[39;00m _minimum_jaxlib_version_str\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions."
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def count_and_get_top_tokens(file_path, model_name, top_n=30):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "    token_counts = Counter(tokens)\n",
    "\n",
    "    top_tokens = token_counts.most_common(top_n)\n",
    "\n",
    "    top_words = [(token, count) for token, count in top_tokens]\n",
    "\n",
    "    return top_words\n",
    "\n",
    "file_path = r'D:\\assignment 2\\answer\\Question 1\\COMBINE_TEXT.txt'\n",
    "model_name = 'bert-base-uncased'\n",
    "top_words = count_and_get_top_tokens(file_path, model_name)\n",
    "\n",
    "for token, count in top_words:\n",
    "    print(f'{token}: {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c960bb9e-f1ac-4927-947d-677df0314d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\spacy\\language.py:2141: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "nlp.max_length = 133328033\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\"diseases\": [], \"drugs\": []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if hasattr(ent,'label_'):\n",
    "            if ent.label_ == \"DISEASE\":\n",
    "                entities[\"diseases\"].append(ent.text)\n",
    "            elif ent.label_ == \"DRUG\":\n",
    "                entities[\"drugs\"].append(ent.text)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "file_path = r'D:\\assignment 2\\answer\\Question 1\\COMBINE_TEXT.txt'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "chunk_size = 5000\n",
    "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "result = {\"diseases\": [], \"drugs\": []}\n",
    "\n",
    "for chunk in chunks:\n",
    "    entities = extract_entities(chunk)\n",
    "    result[\"diseases\"].extend(entities[\"diseases\"])\n",
    "    result[\"drugs\"].extend(entities[\"drugs\"])\n",
    " \n",
    "print(\"Diseases:\", result[\"diseases\"])\n",
    "print(\"Drugs:\", result[\"drugs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"monologg/biobert_v1.1_pubmed\", num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/biobert_v1.1_pubmed\")\n",
    "\n",
    "labels = [\"O\", \"B-Disease\", \"I-Disease\"]\n",
    "\n",
    "def extract_entities(text):\n",
    "    \n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, dim=2).squeeze().tolist()\n",
    "\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "\n",
    "    for token, label_id in zip(tokens, predictions):\n",
    "        label = labels[label_id]\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\"label\": label[2:], \"text\": [token]}\n",
    "        elif label.startswith(\"I-\"):\n",
    "            if current_entity is not None:\n",
    "                current_entity[\"text\"].append(token)\n",
    "        else:\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = None\n",
    "\n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "\n",
    "    extracted_entities = [{\"label\": entity[\"label\"], \"text\": \" \".join(entity[\"text\"])} for entity in entities]\n",
    "    \n",
    "    return extracted_entities\n",
    "\n",
    "file_path = r'D:\\assignment 2\\answer\\Question 1\\COMBINE_TEXT.txt'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "result = extract_entities(text)\n",
    "\n",
    "diseases = [entity[\"text\"] for entity in result if entity[\"label\"] == \"Disease\"]\n",
    "drugs = [entity[\"text\"] for entity in result if entity[\"label\"] == \"Drug\"]\n",
    "\n",
    "print(\"Diseases:\", diseases)\n",
    "print(\"Drugs:\", drugs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
